{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht0pB7800sYa"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVZKJ0Ln09Vf",
        "outputId": "c02f80a1-1372-4d53-cc6f-99fe1a39d974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MkqvsFA30sYb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import roc_curve\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import sklearn\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import gc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zBnWePU2Ncy",
        "outputId": "67486057-f5c4-42b1-e266-a97f71677de1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFzXTv4d0sYd"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S_LzhXA60sYd"
      },
      "outputs": [],
      "source": [
        "## Global Variables\n",
        "'''\n",
        "M : Length of each sequence\n",
        "'''\n",
        "\n",
        "M = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rKZIgfUM0sYd"
      },
      "outputs": [],
      "source": [
        "ROOT = \"../../../Dataset/Raw_Temp\"\n",
        "# ROOT = \"/content/drive/MyDrive/Atharva/StonyBrook_json_format/Raw_Temp\"\n",
        "# ROOT2 = \"/content/drive/MyDrive/Atharva/Notebooks\"\n",
        "# ROOT3 = \"/content/drive/MyDrive/Thesis-Cloud\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf2ApdE70sYe",
        "outputId": "92da7f58-e8eb-46be-f2ba-37815d19fb76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34\n",
            "37\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "with open(os.path.join(ROOT,'fixed_data.txt'),'r') as f :\n",
        "  fixed_data = json.load(f)\n",
        "\n",
        "print(len(fixed_data.keys()))\n",
        "\n",
        "with open(os.path.join(ROOT,'free_data.txt'),'r') as f :\n",
        "  free_data = json.load(f)\n",
        "\n",
        "# with open(os.path.join(ROOT,'Raw_Temp_Gay_Marriage_Fixed.json')) as f :\n",
        "#   gay_marriage_fixed = json.load(f)\n",
        "\n",
        "# with open(os.path.join(ROOT,'Raw_Temp_Gay_Marriage_Free.json')) as f :\n",
        "#   gay_marriage_free = json.load(f)\n",
        "\n",
        "# with open(os.path.join(ROOT,'Raw_Temp_Gun_Control_Fixed.json')) as f :\n",
        "#   gun_control_fixed = json.load(f)\n",
        "\n",
        "# with open(os.path.join(ROOT,'Raw_Temp_Gun_Control_Free.json')) as f :\n",
        "#   gun_control_free = json.load(f)\n",
        "\n",
        "# with open(os.path.join(ROOT,'Raw_Temp_Restaurant_Review_Fixed.json')) as f :\n",
        "#   rest_fixed = json.load(f)\n",
        "\n",
        "# with open(os.path.join(ROOT,'Raw_Temp_Restaurant_Review_Free.json')) as f :\n",
        "#   rest_free = json.load(f)\n",
        "\n",
        "with open(os.path.join(ROOT,'LLM_fixed.json')) as f :\n",
        "  llm_fixed_data = json.load(f)\n",
        "\n",
        "with open(os.path.join(ROOT,'LLM_free.json')) as f :\n",
        "  llm_free_data = json.load(f)\n",
        "\n",
        "print(len(free_data.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ca_6mLUy0sYe"
      },
      "outputs": [],
      "source": [
        "# with open(os.path.join(ROOT,'Buffalo_Fixed.json')) as f :\n",
        "#   buffalo_fixed = json.load(f)\n",
        "\n",
        "# with open(os.path.join(ROOT,'Buffalo_Free.json')) as f :\n",
        "#   buffalo_free = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6I0Hi1a0sYe"
      },
      "source": [
        "Dividing into batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3hFaxvGT0sYe"
      },
      "outputs": [],
      "source": [
        "## We divide each user's keystroke into batches of size 15.\n",
        "\n",
        "def divide_into_batches(x) :\n",
        "  num = len(x) // M\n",
        "  list_text = []\n",
        "\n",
        "  start = 0\n",
        "\n",
        "  for i in range(num):\n",
        "    start = int(i*len(x)/num)\n",
        "    end = int((i+1)*len(x)/num)\n",
        "    temp = x[start:min(end,len(x))]\n",
        "\n",
        "    if len(temp) < 0.8*M :\n",
        "      continue\n",
        "      print(\"Too Small\",len(temp))\n",
        "\n",
        "    list_text.append(temp)\n",
        "\n",
        "  return list_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5JkAGcc20sYf"
      },
      "outputs": [],
      "source": [
        "# def process_data_buffalo(data,verbose=0) :\n",
        "#   for key in data.keys() :\n",
        "#     if verbose == 1 :\n",
        "#       data[key] = json.loads(data[key])\n",
        "\n",
        "#     # print(data[key])\n",
        "\n",
        "#     timestamp_kd = []\n",
        "#     timestamp_ku = []\n",
        "#     list_ = data[key][\"keyboard_data\"]\n",
        "\n",
        "#     for i in range(len(data[key][\"keyboard_data\"])) :\n",
        "#       if list_[i][1].lower() == \"kd\" :\n",
        "#         timestamp_kd.append(int(list_[i][2]) - int(list_[i-1][2]) if i > 0 else int(list_[i][2]))\n",
        "#       else :\n",
        "#         # print(list_[i])\n",
        "#         # print(list_[i][2],list_[i-1][2])\n",
        "#         timestamp_ku.append(int(list_[i][2]) - int(list_[i-1][2]) if i > 0 else int(list_[i][2]))\n",
        "\n",
        "#     timestamp_kd = np.array(timestamp_kd, dtype=np.float32)\n",
        "#     timestamp_ku = np.array(timestamp_ku, dtype=np.float32)\n",
        "\n",
        "#     if len(timestamp_kd) > 0 :\n",
        "#       ## Min Max Scaling\n",
        "#       timestamp_kd = (timestamp_kd - min(timestamp_kd))/(max(timestamp_kd) - min(timestamp_kd))\n",
        "\n",
        "#     if len(timestamp_ku) > 0 :\n",
        "#       # print(timestamp_ku)\n",
        "#       timestamp_ku = (timestamp_ku - min(timestamp_ku))/(max(timestamp_ku) - min(timestamp_ku))\n",
        "#       # print(timestamp_ku)\n",
        "\n",
        "#     ku_count = 0\n",
        "#     kd_count = 0\n",
        "\n",
        "#     for i in range(len(data[key][\"keyboard_data\"])) :\n",
        "#       ## Swap 0 element with 1 element\n",
        "#       temp = list_[i][0]\n",
        "#       list_[i][0] = list_[i][1]\n",
        "#       list_[i][1] = temp\n",
        "\n",
        "#       if list_[i][0].lower() == \"ku\" :\n",
        "#         # print(list_[i][2],timestamp_ku[ku_count])\n",
        "#         list_[i][2] = float(timestamp_ku[ku_count])\n",
        "#         ku_count += 1\n",
        "#       else :\n",
        "#         list_[i][2] = float(timestamp_kd[kd_count])\n",
        "#         kd_count += 1\n",
        "\n",
        "#     data[key][\"keyboard_data\"] = list_[1:]\n",
        "#     data[key] = divide_into_batches(data[key][\"keyboard_data\"])\n",
        "\n",
        "#   return data\n",
        "\n",
        "# buffalo_free = process_data_buffalo(buffalo_free)\n",
        "# buffalo_fixed = process_data_buffalo(buffalo_fixed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iFTDJCYf0sYf"
      },
      "outputs": [],
      "source": [
        "def process_data(data,verbose=0) :\n",
        "  for key in data.keys() :\n",
        "    if verbose == 1 :\n",
        "      data[key] = json.loads(data[key])\n",
        "\n",
        "    timestamp_kd = []\n",
        "    timestamp_ku = []\n",
        "    list_ = data[key][\"keyboard_data\"]\n",
        "\n",
        "    for i in range(len(data[key][\"keyboard_data\"])) :\n",
        "      if list_[i][0] == \"KD\" :\n",
        "        timestamp_kd.append(list_[i][2] - list_[i-1][2] if i > 0 else list_[i][2])\n",
        "      else :\n",
        "        timestamp_ku.append(list_[i][2] - list_[i-1][2] if i > 0 else list_[i][2])\n",
        "\n",
        "    timestamp_kd = np.array(timestamp_kd, dtype=np.float32)\n",
        "    timestamp_ku = np.array(timestamp_ku, dtype=np.float32)\n",
        "\n",
        "    if len(timestamp_kd) > 0 :\n",
        "      ## Min Max Scaling\n",
        "      timestamp_kd = (timestamp_kd - min(timestamp_kd))/(max(timestamp_kd) - min(timestamp_kd))\n",
        "\n",
        "    if len(timestamp_ku) > 0 :\n",
        "      timestamp_ku = (timestamp_ku - min(timestamp_ku))/(max(timestamp_ku) - min(timestamp_ku))\n",
        "\n",
        "    ku_count = 0\n",
        "    kd_count = 0\n",
        "\n",
        "    for i in range(len(data[key][\"keyboard_data\"])) :\n",
        "      if list_[i][0] == \"KU\" :\n",
        "        list_[i][2] = float(timestamp_ku[ku_count])\n",
        "        ku_count += 1\n",
        "      else :\n",
        "        list_[i][2] = float(timestamp_kd[kd_count])\n",
        "        kd_count += 1\n",
        "\n",
        "    data[key][\"keyboard_data\"] = list_[1:]\n",
        "    data[key] = divide_into_batches(data[key][\"keyboard_data\"])\n",
        "\n",
        "  return data\n",
        "\n",
        "# gm_fixed = process_data(gay_marriage_fixed)\n",
        "# gm_free = process_data(gay_marriage_free)\n",
        "# gnc_fixed = process_data(gun_control_fixed)\n",
        "# gnc_free = process_data(gun_control_free)\n",
        "# rs_free = process_data(rest_fixed)\n",
        "# rs_fixed = process_data(rest_free)\n",
        "free_data = process_data(free_data,1)\n",
        "fixed_data = process_data(fixed_data,1)\n",
        "llm_free_data = process_data(llm_free_data)\n",
        "llm_fixed_data = process_data(llm_fixed_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USQzi6t-0sYg"
      },
      "source": [
        "Train-Test Dataset Formation & Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dOVQqZYB0sYg"
      },
      "outputs": [],
      "source": [
        "def update_dict(adder, addee):\n",
        "  \"\"\"\n",
        "  Updates a dictionary by adding the values from another dictionary.\n",
        "\n",
        "  Parameters:\n",
        "  - adder (dict): The dictionary to be updated.\n",
        "  - addee (dict): The dictionary whose values will be added to the `adder` dictionary.\n",
        "\n",
        "  Returns:\n",
        "  - dict: The updated dictionary.\n",
        "  \"\"\"\n",
        "\n",
        "  for key in addee:\n",
        "    if key in adder:\n",
        "      adder[key].extend(addee[key])\n",
        "    else:\n",
        "      adder[key] = addee[key]\n",
        "\n",
        "  return adder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hTTF9olP0sYg"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    We can create any combination of datasets for training and\n",
        "    testing in this pipeline to create the training and testing sets.\n",
        "\n",
        "    fixed_data, free_data --> proposed dataset\n",
        "    buffalo_fixed, buffalo_free --> buffalo dataset\n",
        "    gm_fixed, gm_free --> SBU (Gay Marriage)\n",
        "    gnc_fixed, gnc_free --> SBU (Gun Control)\n",
        "    rs_fixed, rs_free --> SBU (Restaurant Reviews)\n",
        "'''\n",
        "\n",
        "## Train : SBU Dataset + Buffalo Dataset, Test: Proposed Dataset\n",
        "fixed_data_test = {}\n",
        "free_data_test = {}\n",
        "\n",
        "# fixed_data_test.update(gm_fixed)\n",
        "# free_data_test.update(gm_free)\n",
        "\n",
        "fixed_data_train = {}\n",
        "free_data_train = {}\n",
        "\n",
        "# fixed_data_train.update(gm_fixed)\n",
        "fixed_data_train = update_dict(fixed_data_train,llm_fixed_data)\n",
        "fixed_data_train = update_dict(fixed_data_train, fixed_data)\n",
        "# fixed_data_train = update_dict(fixed_data_train,buffalo_fixed)\n",
        "\n",
        "# free_data_train.update(gm_free)\n",
        "free_data_train = update_dict(free_data_train,llm_free_data)\n",
        "free_data_train = update_dict(free_data_train, free_data)\n",
        "# free_data_train = update_dict(free_data_train,gnc_free)\n",
        "# free_data_train = update_dict(free_data_train,buffalo_free)\n",
        "\n",
        "## For specific case combine the datasets into the training set only. Leave the test set empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CZCyBlaa0sYh"
      },
      "outputs": [],
      "source": [
        "##Key board map to map each key to a number\n",
        "keyboard_map = {\n",
        "    'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10,\n",
        "    'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20,\n",
        "    'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26,\n",
        "    '0': 27, 'd0' : 27, 'd1' : 28, '1': 28, '2': 29, 'd2': 29, '3': 30, 'd3': 30, '4': 31, 'd4' : 31,'5': 32, 'd5':32, '6': 33, 'd6':33, '7': 34, 'd7': 34, '8': 35, 'd8': 35, '9': 36, 'd9':36,\n",
        "    'f1': 37, 'f2': 38, 'f3': 39, 'f4': 40, 'f5': 41, 'f6': 42, 'f7': 43, 'f8': 44, 'f9': 45, 'f10': 46,\n",
        "    'f11': 47, 'f12': 48, 'esc': 49, '`': 50, '-': 51, 'subtract':51, '=': 52, 'backspace': 53,'back':53, 'tab': 54, '[': 55, ']': 56,\n",
        "    '\\\\': 57, 'capslock': 58,'capital':58, ';': 59, '\\'': 60, 'enter': 61, 'return':61, 'shift': 62, ',': 63, '.': 64, 'decimal':64, 'oemperiod': 64 , '/': 65 ,'divide':65, 'control': 66,'ctrl':66,\n",
        "    'alt': 67, ' ': 68, 'printscreen': 69, 'scrolllock': 70,'scroll':70, 'pause': 71, 'insert': 72, 'home': 73, 'pageup': 74,\n",
        "    'delete': 75, 'end': 76, 'pagedown': 77, 'arrowup': 78, 'arrowleft': 79, 'arrowdown': 80, 'arrowright': 81,\n",
        "    'numlock': 82, 'numpad0': 83, 'numpad1': 84, 'numpad2': 85, 'numpad3': 86, 'numpad4': 87, 'numpad5': 88,\n",
        "    'numpad6': 89, 'numpad7': 90, 'numpad8': 91, 'numpad9': 92, 'numpadmultiply': 93, 'numpadadd': 94,\n",
        "    'numpadsubtract': 95, 'numpaddecimal': 96, 'numpaddivide': 97, 'numpadenter': 98, 'contextmenu': 99,\n",
        "    'leftctrl': 100, 'leftshift': 101, 'leftshiftkey':101, 'leftalt': 102, 'lmenu':102, 'leftmeta': 103, 'rightctrl': 104,'rcontrolkey':104, 'rightshift': 105, 'rshiftkey': 105,\n",
        "    'rightalt': 106, 'rmenu':106, 'rightmeta': 107, ':': 108, 'colon': 108, 'unidentified':0, ')': 109, '(':110, 'meta':111, '≠':112, '@':113, '>':114,'<':115, '*':116,'+':117,'add' : 117,'#':118,'$':119, '\"':120, 'process':121,'_':122,\n",
        "    '{':123,'}':124,'?':134,'f1':135,'f2':136,'f3':137,'f4':138,'f5':139,'f6':140,'f7':141,'f14':142,'´':143,'':0,'©':144,'escape':49,'clear':145,'lcontrolkey':100,'lshiftkey':101, 'space':68, 'left': 146, 'right': 147, 'up':148, 'down': 149, 'apps':150,\n",
        "    'rwin' : 151, 'next': 152, 'lwin' : 153, 'browserback':154, 'browserforward':155, 'browserrefresh':156, 'browserstop':157, 'browsersearch':158, 'browserfavorites':159, 'browserhome':160, 'volumemute':161, 'volumedown':162, 'volumeup':163, 'medianexttrack':164,\n",
        "    'mediaprevioustrack':165, 'audiovolumemute':166, '!':167, '&':168, '|':169, 'altgraph': 170, '%':171, 'audiovolumeup':172, '^':173, '~':174\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rZK3yKKw0sYh"
      },
      "outputs": [],
      "source": [
        "## Replace the character with the ascii value, KD : 0, KU : 1, timestamp : relative\n",
        "def convert_list(list_) :\n",
        "  \"\"\"\n",
        "  Converts a list of key events into a transformed list.\n",
        "\n",
        "  Args:\n",
        "    list_ (list): A list of key events, where each event is represented as a tuple of three elements:\n",
        "                  the key action ('KD' for Key Down or 'KU' for Key Up), the key value, and the timestamp.\n",
        "\n",
        "  Returns:\n",
        "    list: A transformed list where each event is represented as a list with the following elements:\n",
        "          - 0 for Key Down or 1 for Key Up\n",
        "          - The normalized value of the key (if applicable)\n",
        "          - The timestamp difference between the current event and the previous event\n",
        "\n",
        "  \"\"\"\n",
        "  start_time = 0\n",
        "  trans_list = []\n",
        "  timestamp_list_kd = []\n",
        "  timestamp_list_ku = []\n",
        "  shift_count = 0\n",
        "\n",
        "  for i in range(len(list_)) :\n",
        "    temp = []\n",
        "\n",
        "    ## Assigning value to Key Up and Key Down\n",
        "    if list_[i][0] == 'KD' :\n",
        "      temp.append(0)\n",
        "    else:\n",
        "      temp.append(1)\n",
        "\n",
        "    # print(str(list_[i][1]).lower())\n",
        "\n",
        "    ## Convert to ascii value\n",
        "    if (len(str(list_[i][1]).lower()) > 1 and isinstance(list_[i][1],int)) : continue\n",
        "    elif str(list_[i][1]).lower().find(\"oem\") != -1 or str(list_[i][1]).lower().find(\"lbutton,\") != -1:\n",
        "      continue\n",
        "    else :\n",
        "      temp.append(keyboard_map[str(list_[i][1]).lower()])\n",
        "\n",
        "    if str(list_[i][1]).lower() == 'shift' :\n",
        "      shift_count += 1\n",
        "\n",
        "    ## Store the diff in timestamp\n",
        "    if i>=0 :\n",
        "      temp.append(float(list_[i][2]))\n",
        "\n",
        "    if shift_count < len(list_)*0.2 :\n",
        "      trans_list.append(temp)\n",
        "\n",
        "  return trans_list\n",
        "\n",
        "def convert_data(data):\n",
        "  \"\"\"\n",
        "  Convert the given data dictionary into a new dictionary with converted lists.\n",
        "\n",
        "  Args:\n",
        "    data (dict): The input data dictionary.\n",
        "\n",
        "  Returns:\n",
        "    dict: The converted data dictionary.\n",
        "\n",
        "  \"\"\"\n",
        "  p_data = {}\n",
        "\n",
        "  for key in data.keys():\n",
        "    list_compiled = []\n",
        "\n",
        "    for list_ in data[key]:\n",
        "      temp = convert_list(list_)\n",
        "\n",
        "      if len(temp) > 0:\n",
        "        list_compiled.append(temp)\n",
        "\n",
        "    if len(list_compiled) > 0:\n",
        "      p_data[key] = list_compiled\n",
        "\n",
        "  return p_data\n",
        "\n",
        "\n",
        "## Converting data\n",
        "free_data_train = convert_data(free_data_train)\n",
        "fixed_data_train = convert_data(fixed_data_train)\n",
        "free_data_test = convert_data(free_data_test)\n",
        "fixed_data_test = convert_data(fixed_data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VwoQ8rPy0sYh"
      },
      "outputs": [],
      "source": [
        "## Create validation set\n",
        "free_data_val = {}\n",
        "fixed_data_val = {}\n",
        "\n",
        "\n",
        "for key in free_data_train.keys() :\n",
        "    if len(free_data_train[key]) > 1 :\n",
        "        val_sect = int(0.1*len(free_data_train[key]))\n",
        "        free_data_val[key] = free_data_train[key][:val_sect]\n",
        "        free_data_test[key] = free_data_train[key][val_sect:3*val_sect]\n",
        "        free_data_train[key] = free_data_train[key][3*val_sect:]\n",
        "\n",
        "\n",
        "for key in fixed_data_train.keys() :\n",
        "    if len(fixed_data_train[key]) > 1 :\n",
        "        fixed_data_val[key] = fixed_data_train[key][:int(0.1*len(fixed_data_train[key]))]\n",
        "        fixed_data_test[key] = fixed_data_train[key][int(0.1*len(fixed_data_train[key])):int(0.3*len(fixed_data_train[key]))]\n",
        "        fixed_data_train[key] = fixed_data_train[key][int(0.3*len(fixed_data_train[key])):]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GuJwXs1C0sYi"
      },
      "outputs": [],
      "source": [
        "## Padding and clipping sequences\n",
        "\n",
        "mask_free_train = {}\n",
        "mask_fixed_train = {}\n",
        "mask_fixed_val = {}\n",
        "mask_free_val = {}\n",
        "mask_free_test = {}\n",
        "mask_fixed_test = {}\n",
        "\n",
        "def pad_clip_seq(x) :\n",
        "  curr_mask = [1]*len(x)\n",
        "\n",
        "  if(len(x) > M) :\n",
        "    ## If length is greater than the sequence length M : Clip the sequence\n",
        "\n",
        "    x = x[:M]\n",
        "    curr_mask = curr_mask[:M]\n",
        "\n",
        "  ## If length is less than the sequence length M : Pad the sequence\n",
        "  for i in range(max(0,M-len(x))) :\n",
        "    x.append([-1,-1,-1])\n",
        "    curr_mask.append(0)\n",
        "\n",
        "  return x,curr_mask\n",
        "\n",
        "def return_pad_seq(data):\n",
        "  \"\"\"\n",
        "  Pad sequences in the given data dictionary and return the padded sequences along with the corresponding masks.\n",
        "\n",
        "  Args:\n",
        "    data (dict): A dictionary containing sequences to be padded.\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing the padded sequences and their corresponding masks.\n",
        "\n",
        "  \"\"\"\n",
        "  mask = {}\n",
        "\n",
        "  for key in data.keys():\n",
        "    new_list = []\n",
        "    mask_l = []\n",
        "\n",
        "    for list_ in data[key]:\n",
        "      list_, curr_mask = pad_clip_seq(list_)\n",
        "\n",
        "      new_list.append(list_)\n",
        "      mask_l.append(curr_mask)\n",
        "\n",
        "    data[key] = new_list\n",
        "    mask[key] = mask_l\n",
        "\n",
        "  return data, mask\n",
        "\n",
        "free_data_train,mask_free_train = return_pad_seq(free_data_train)\n",
        "fixed_data_train,mask_fixed_train = return_pad_seq(fixed_data_train)\n",
        "fixed_data_val,mask_fixed_val = return_pad_seq(fixed_data_val)\n",
        "free_data_val,mask_free_val = return_pad_seq(free_data_val)\n",
        "free_data_test,mask_free_test = return_pad_seq(free_data_test)\n",
        "fixed_data_test,mask_fixed_test = return_pad_seq(fixed_data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1DO66BN0sYi"
      },
      "source": [
        "Combining Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7DHw6UIw0sYi"
      },
      "outputs": [],
      "source": [
        "## Combine Pairs of a given set of fixed and free data\n",
        "def combine_pairs(fixed_data,free_data,mask_fixed,mask_free) :\n",
        "  data = {}\n",
        "  mask = {}\n",
        "  y_data = {}\n",
        "\n",
        "  for key in fixed_data.keys() :\n",
        "    if key not in free_data.keys() :\n",
        "      continue\n",
        "    else:\n",
        "      data[key] = []\n",
        "      mask[key] = []\n",
        "      y_data[key] = []\n",
        "\n",
        "      ## For each user, we create pairs of fixed and free data, Label : 1\n",
        "      for fixed_index in range(len(fixed_data[key])) :\n",
        "        for free_index in range(len(free_data[key])) :\n",
        "          data[key].append([fixed_data[key][fixed_index],free_data[key][free_index]])\n",
        "          mask[key].append([mask_fixed[key][fixed_index],mask_free[key][free_index]])\n",
        "          y_data[key].append(1)\n",
        "\n",
        "        for fixed_index_2 in range(len(fixed_data[key])) :\n",
        "          if fixed_data[key][fixed_index_2] == fixed_data[key][fixed_index] or ([fixed_data[key][fixed_index_2],fixed_data[key][fixed_index]] in data[key]) or ([fixed_data[key][fixed_index],fixed_data[key][fixed_index_2]] in data[key]):\n",
        "            continue\n",
        "          else :\n",
        "            data[key].append([fixed_data[key][fixed_index],fixed_data[key][fixed_index_2]])\n",
        "            mask[key].append([mask_fixed[key][fixed_index],mask_fixed[key][fixed_index_2]])\n",
        "            y_data[key].append(0)\n",
        "\n",
        "      ## For each user, we create pairs of fixed and free data, Label : 0\n",
        "      for free_index in range(len(free_data[key])) :\n",
        "        for free_index_2 in range(len(free_data[key])) :\n",
        "          if free_data[key][free_index_2] == free_data[key][free_index] or ([free_data[key][free_index_2],free_data[key][free_index]] in data[key]) or ([free_data[key][free_index],free_data[key][free_index_2]] in data[key]):\n",
        "            continue\n",
        "          else :\n",
        "            data[key].append([free_data[key][free_index],free_data[key][free_index_2]])\n",
        "            mask[key].append([mask_free[key][free_index],mask_free[key][free_index_2]])\n",
        "            y_data[key].append(0)\n",
        "\n",
        "  return data,y_data,mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "92aQED6V0sYj"
      },
      "outputs": [],
      "source": [
        "new_data_train,y_data_train,mask_train = combine_pairs(fixed_data_train,free_data_train,mask_fixed_train,mask_free_train)\n",
        "new_data_val,y_data_val,mask_val = combine_pairs(fixed_data_val,free_data_val,mask_fixed_val,mask_free_val)\n",
        "new_data_test,y_data_test,mask_test = combine_pairs(fixed_data_test,free_data_test,mask_fixed_test,mask_free_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lNEIHZUj0sYj"
      },
      "outputs": [],
      "source": [
        "## Convert the data into a single list\n",
        "\n",
        "def combine(new_data, mask, y_data):\n",
        "  \"\"\"\n",
        "  Combines the data, mask, and y_data into lists.\n",
        "\n",
        "  Args:\n",
        "    new_data (dict): A dictionary containing the new data.\n",
        "    mask (dict): A dictionary containing the mask.\n",
        "    y_data (dict): A dictionary containing the y_data.\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing the combined data list, combined mask list, and y_data list.\n",
        "  \"\"\"\n",
        "  combined_data_list = []\n",
        "  combined_mask_list = []\n",
        "  y_data_list = []\n",
        "\n",
        "  for key in new_data.keys():\n",
        "    if len(combined_data_list) == 0:\n",
        "      combined_data_list = new_data[key]\n",
        "      combined_mask_list = mask[key]\n",
        "      y_data_list = y_data[key]\n",
        "    else:\n",
        "      combined_data_list.extend(new_data[key])\n",
        "      combined_mask_list.extend(mask[key])\n",
        "      y_data_list.extend(y_data[key])\n",
        "\n",
        "  return combined_data_list, combined_mask_list, y_data_list\n",
        "\n",
        "combined_data_list_train,combined_mask_list_train,y_data_list_train = combine(new_data_train,mask_train,y_data_train)\n",
        "combined_data_list_val,combined_mask_list_val,y_data_list_val = combine(new_data_val,mask_val,y_data_val)\n",
        "combined_data_list_test,combined_mask_list_test,y_data_list_test = combine(new_data_test,mask_test,y_data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cobbB4Ks0sYj",
        "outputId": "69337871-243f-4662-f658-c37d5658007d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([0, 1]), array([487969, 471183]))\n"
          ]
        }
      ],
      "source": [
        "## Number of unique samples in training set and their distribution\n",
        "\n",
        "len(combined_data_list_train)\n",
        "print(np.unique(y_data_list_train,return_counts=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzN99y8q0sYk"
      },
      "source": [
        "Class Balancing for Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ6efgQD0sYl",
        "outputId": "5d6de8c5-9b99-4825-e698-b3ccd74e34fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "indexes = np.arange(len(combined_data_list_train))\n",
        "y_data_list_train = np.array(y_data_list_train)\n",
        "class_0_index = indexes[y_data_list_train == 0]\n",
        "class_1_index = indexes[y_data_list_train == 1]\n",
        "\n",
        "# print(len(class_1_index))\n",
        "min_length = min(len(class_0_index),len(class_1_index))\n",
        "\n",
        "indexes = np.concatenate((class_0_index[:min_length],class_1_index[:min_length]))\n",
        "random.shuffle(indexes)\n",
        "\n",
        "combined_data_list_train = np.array(combined_data_list_train)\n",
        "combined_mask_list_train = np.array(combined_mask_list_train)\n",
        "y_data_list_train = np.array(y_data_list_train)\n",
        "\n",
        "combined_data_list_train = combined_data_list_train[indexes]\n",
        "combined_mask_list_train = combined_mask_list_train[indexes]\n",
        "y_data_list_train = y_data_list_train[indexes]\n",
        "\n",
        "del indexes,class_0_index,class_1_index\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WNyW1_G0sYl",
        "outputId": "47641687-9f69-4d08-ccef-eabce21f7b26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([0, 1]), array([471183, 471183]))\n"
          ]
        }
      ],
      "source": [
        "print(np.unique(y_data_list_train,return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_yOqsUHP0sYl"
      },
      "outputs": [],
      "source": [
        "## Creating Data Loader\n",
        "\n",
        "class CustomDataset(Dataset) :\n",
        "  def __init__(self,data,mask,label) :\n",
        "    self.data = data\n",
        "    self.mask = mask\n",
        "    self.label = label\n",
        "\n",
        "  def __getitem__(self,index) :\n",
        "    x = torch.tensor(self.data[index],dtype=torch.float32)\n",
        "    m = torch.tensor(self.mask[index],dtype=torch.float32)\n",
        "\n",
        "    counts = np.unique(m[0,:],return_counts=True)\n",
        "    length1 = counts[1][-1]\n",
        "    counts = np.unique(m[1,:],return_counts=True)\n",
        "    length2 = counts[1][-1]\n",
        "\n",
        "    temp = torch.zeros(2)\n",
        "    temp[self.label[index]] = 1\n",
        "\n",
        "    return x[0,:],x[1,:],self.label[index],length1,length2\n",
        "\n",
        "  def __len__(self) :\n",
        "    return len(self.data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jrTfKRa30sYl"
      },
      "outputs": [],
      "source": [
        "## Config\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5Nx5rsLw0sYl"
      },
      "outputs": [],
      "source": [
        "train_data = CustomDataset(combined_data_list_train,combined_mask_list_train,y_data_list_train)\n",
        "val_data = CustomDataset(combined_data_list_val,combined_mask_list_val,y_data_list_val)\n",
        "test_data = CustomDataset(combined_data_list_test,combined_mask_list_test,y_data_list_test)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.cuda(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4YrNiGH0sYl"
      },
      "source": [
        "Model & Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hv3OJrZq0sYl"
      },
      "outputs": [],
      "source": [
        "def contrastive_loss(fv1, fv2, y, alpha):\n",
        "    # Move all inputs to the specified device\n",
        "    fv1 = fv1.to(device)\n",
        "    fv2 = fv2.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # Element-wise square of the difference\n",
        "    squared_diff = (fv1 - fv2) ** 2\n",
        "\n",
        "    # Summing the squared differences along the last dimension and taking the square root for Euclidean distance\n",
        "    d = torch.sqrt(torch.sum(squared_diff, dim=-1))\n",
        "\n",
        "    # Element-wise maximum\n",
        "    max_part = torch.clamp_min(alpha - d, min=0) ** 2\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = (((1 - y) * d ** 2) / 2) + ((y * max_part) / 2)\n",
        "    return torch.mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tWgCtSYn0sYl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TypeNet(nn.Module):\n",
        "    def __init__(self, sequence_length, in_dim, hidden_dim_1, hidden_dim_2, output_dim, dropout):\n",
        "        super(TypeNet, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.lstm1 = nn.LSTM(input_size=in_dim, hidden_size=hidden_dim_1, batch_first=True, dropout=0.2)\n",
        "        self.lstm2 = nn.LSTM(input_size=hidden_dim_1, hidden_size=hidden_dim_2, batch_first=True, dropout=0.2)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=in_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(num_features=hidden_dim_1)\n",
        "        self.bn3 = nn.BatchNorm1d(num_features=hidden_dim_2*sequence_length)\n",
        "        self.bn4 = nn.BatchNorm1d(num_features=output_dim)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.dropout_1 = nn.Dropout(p=dropout)\n",
        "        self.dropout_2 = nn.Dropout(p=dropout)\n",
        "        self.fc1 = nn.Linear(sequence_length*hidden_dim_2, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Weight initialization\n",
        "        for name, param in self.lstm1.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                nn.init.xavier_uniform_(param.data)\n",
        "            elif 'weight_hh' in name:\n",
        "                nn.init.kaiming_uniform_(param.data)\n",
        "        for name, param in self.lstm2.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                nn.init.xavier_uniform_(param.data)\n",
        "            elif 'weight_hh' in name:\n",
        "                nn.init.kaiming_uniform_(param.data)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "\n",
        "    def forward(self, x, length):\n",
        "        x = x.to(device)\n",
        "\n",
        "        x = torch.movedim(x, 2, 1)\n",
        "        out = self.bn1(x)\n",
        "        out = torch.movedim(out, 2, 1)\n",
        "\n",
        "        out = nn.utils.rnn.pack_padded_sequence(out, length, batch_first=True, enforce_sorted=False)\n",
        "        out, _ = self.lstm1(out)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "\n",
        "        out = self.act1(out)\n",
        "        out = self.dropout_1(out)\n",
        "\n",
        "        out = torch.movedim(out, 2, 1)\n",
        "        out = self.bn2(out)\n",
        "        out = torch.movedim(out, 2, 1)\n",
        "\n",
        "        out_p = nn.utils.rnn.pack_padded_sequence(out, length, batch_first=True, enforce_sorted=False)\n",
        "        out_p, _ = self.lstm2(out_p)\n",
        "        out_p, _ = nn.utils.rnn.pad_packed_sequence(out_p, batch_first=True)\n",
        "\n",
        "        out_p = self.act2(out_p)\n",
        "        out_p = torch.reshape(out_p, (out.shape[0], out.shape[1]*out.shape[2]))\n",
        "        out = self.bn3(out_p)\n",
        "        out = self.fc1(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "AMIQg8Qn0sYm"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomTypeNet(nn.Module) :\n",
        "  def __init__(self,sequence_length,in_dim,hidden_dim_1,hidden_dim_2,output_dim,dropout) :\n",
        "    super(CustomTypeNet,self).__init__()\n",
        "    self.tn1 = TypeNet(sequence_length,in_dim,hidden_dim_1,hidden_dim_2,output_dim,dropout)\n",
        "    self.tn2 = TypeNet(sequence_length,in_dim,hidden_dim_1,hidden_dim_2,output_dim,dropout)\n",
        "\n",
        "  def forward(self,x1,x2,length1,length2) :\n",
        "    x1 = self.tn1(x1,length1)\n",
        "    x2 = self.tn2(x2,length2)\n",
        "\n",
        "    return x1,x2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "oJX95DM80sYm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def calculate_eer(d, labels):\n",
        "    # Element-wise square of the difference\n",
        "    with torch.no_grad() :\n",
        "      # squared_diff = (fv1 - fv2) ** 2\n",
        "\n",
        "      # # Summing the squared differences along the last dimension and taking the square root for Euclidean distance\n",
        "      # d = torch.sqrt(torch.sum(squared_diff, dim=1)).detach().cpu().numpy()\n",
        "\n",
        "      # Calculate the False Positive Rates, True Positive Rates, and thresholds\n",
        "      fpr, tpr, thresholds = roc_curve(labels, d, pos_label=1)\n",
        "\n",
        "      # Handle cases where tpr or fpr contains nan values\n",
        "      tpr = np.nan_to_num(tpr)\n",
        "      fpr = np.nan_to_num(fpr)\n",
        "\n",
        "      # Find the EER\n",
        "      eer_threshold = thresholds[np.argmin(np.absolute((1 - tpr) - fpr))]\n",
        "      eer = fpr[np.argmin(np.absolute((1 - tpr) - fpr))]\n",
        "\n",
        "      return eer, eer_threshold, np.average(fpr), np.average(1-tpr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "IUtiEcys0sYm"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWv5URV80sYm"
      },
      "source": [
        "Training Custom Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "EYm2jyEs0sYm"
      },
      "outputs": [],
      "source": [
        "## Model Parameters\n",
        "EPOCHS = 200\n",
        "LR = 0.001\n",
        "DROPOUT = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "IkQDMsup0sYm"
      },
      "outputs": [],
      "source": [
        "now = datetime.now()\n",
        "current_time = now.strftime(\"%H_%M_%S\")\n",
        "new_path = current_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7324_JXB0sYm",
        "outputId": "a972669c-6956-45c6-e73c-bfc5ee0758a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "model = CustomTypeNet(M,3,128,128,128,DROPOUT)\n",
        "model.to(device)\n",
        "loss = nn.BCELoss()\n",
        "\n",
        "# for param in model.parameters() :\n",
        "#   print(param.device)\n",
        "\n",
        "optimizer = Adam(model.parameters(),lr=LR, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "uqspl53F0sYn"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "run = wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=\"Dataset Specific TypeNet(Attention) EER V2\",\n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        \"learning_rate\": LR,\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"BATCH_SIZE\": BATCH_SIZE,\n",
        "        \"M\": M,\n",
        "        \"Dropout\": DROPOUT,\n",
        "        \"Dataset\": \"RF\",\n",
        "        \"Model Path\": new_path\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "tVk0ON0i0sYn"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"18_52_42\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "isp0Zf0O0sYn"
      },
      "outputs": [],
      "source": [
        "is_new_model = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "4QFCksla0sYn"
      },
      "outputs": [],
      "source": [
        "def load_latest_model() :\n",
        "  dir_list = os.listdir(os.path.join(os.getcwd(),checkpoint_path))\n",
        "  num_max = 0\n",
        "\n",
        "  for item in dir_list :\n",
        "    res = [int(i) for i in item if i.isdigit()]\n",
        "    num_max = max(res[0],num_max)\n",
        "\n",
        "  return str(num_max)\n",
        "\n",
        "\n",
        "\n",
        "def load_model() :\n",
        "  print(\"Loading Model Checkpoint ....\")\n",
        "  curr_epoch = load_latest_model()\n",
        "  model_path = \"_model{}.pth\".format(curr_epoch)\n",
        "  print(\"Loading : {}\".format(model_path))\n",
        "  model = torch.load(os.path.join(checkpoint_path,model_path))\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyA2-ZrO0sYn",
        "outputId": "b75d65b2-3c3b-4335-ea30-91f2bd5bf363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH - 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 3879/7363 [03:55<04:04, 14.25it/s]"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
        "from torch.nn.functional import cosine_similarity, binary_cross_entropy\n",
        "from torch.nn import CosineSimilarity\n",
        "\n",
        "loss_plot = []\n",
        "val_loss_plot = []\n",
        "train_eer_loss = []\n",
        "val_eer_loss = []\n",
        "iter = []\n",
        "path = None\n",
        "curr_epoch = 0\n",
        "cos = CosineSimilarity(dim=1)\n",
        "\n",
        "## Loading model from checkpoint\n",
        "if is_new_model :\n",
        "  path = None\n",
        "else:\n",
        "  path = checkpoint_path\n",
        "  model = load_model()\n",
        "  curr_epoch = int(load_latest_model()) + 1\n",
        "\n",
        "\n",
        "for epoch in range(curr_epoch,EPOCHS+curr_epoch) :\n",
        "  train_loss = 0\n",
        "  val_loss = 0\n",
        "  train_count = 0\n",
        "  val_count = 0\n",
        "  acc_train_avg = 0\n",
        "  acc_val_avg = 0\n",
        "  f1_train = 0\n",
        "  f1_val = 0\n",
        "  predicted_train = []\n",
        "  label_train = []\n",
        "  predicted_val = []\n",
        "  label_val = []\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  eer_threshold_train = 0\n",
        "  eer_threshold_val = 0\n",
        "  eer_threshold_test = 0\n",
        "\n",
        "  eer_train = 0\n",
        "  eer_val = 0\n",
        "  eer_test = 0\n",
        "\n",
        "  fpr_train = 0\n",
        "  fpr_val = 0\n",
        "  fpr_test = 0\n",
        "\n",
        "  fnr_train = 0\n",
        "  fnr_val = 0\n",
        "  fnr_test = 0\n",
        "\n",
        "  print(\"EPOCH - {}\".format(epoch))\n",
        "\n",
        "  ### Training Loop ###\n",
        "\n",
        "  for item in tqdm(train_loader) :\n",
        "    ## Train Model\n",
        "    # try:\n",
        "      model.train()\n",
        "      x1,x2,label,length1,length2 = item\n",
        "      x1 = x1.to(device)\n",
        "      x2 = x2.to(device)\n",
        "      label = label.to(device)\n",
        "      label = label.float()\n",
        "      optimizer.zero_grad()\n",
        "      # print(x1.shape)\n",
        "      output = model(x1,x2,length1,length2)\n",
        "      # print(output[0].shape)\n",
        "      d = (cos(output[0],output[1]) + 1)/2\n",
        "      loss_ = loss(d,label)\n",
        "      eer_result = calculate_eer(d.detach().cpu().numpy(),np.array(label.cpu()))\n",
        "      eer_train += eer_result[0]/output[0].shape[0]\n",
        "      fpr_train += eer_result[2]\n",
        "      fnr_train += eer_result[3]\n",
        "\n",
        "      eer_threshold_train = eer_result[1]\n",
        "\n",
        "      train_loss += loss_.item()\n",
        "      loss_.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print(curr_pred)\n",
        "\n",
        "      curr_pred = (d.detach().cpu().numpy() >= 0.50)*1\n",
        "      curr_label = label.detach().cpu().numpy()\n",
        "      predicted_train.extend(curr_pred)\n",
        "      label_train.extend(curr_label)\n",
        "\n",
        "      ## Accuracy Calculation\n",
        "      acc_curr = np.sum(curr_pred == curr_label)/output[0].shape[0]\n",
        "      acc_train_avg += acc_curr\n",
        "\n",
        "      ## Increase Count\n",
        "      train_count += 1\n",
        "\n",
        "      ## Delete variables\n",
        "      del output,x1,x2,length1,length2,label,loss_,curr_pred,curr_label,d,eer_result\n",
        "\n",
        "    # except Exception as e:\n",
        "    #   continue\n",
        "\n",
        "  ### Validation Loop ###\n",
        "\n",
        "  with torch.no_grad() :\n",
        "    # try:\n",
        "      for item in tqdm(val_loader) :\n",
        "      ## Testing Model\n",
        "        x1,x2,label,length1,length2 = item\n",
        "        x1 = x1.to(device)\n",
        "        x2 = x2.to(device)\n",
        "        label = label.to(device)\n",
        "        label = label.float()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x1,x2,length1,length2)\n",
        "        d = (cosine_similarity(output[0],output[1]) + 1)/2\n",
        "        # print(d)\n",
        "        loss_ = loss(d,label)\n",
        "        val_loss += torch.mean(loss_).item()\n",
        "\n",
        "        eer_result = calculate_eer(d.detach().cpu().numpy(),np.array(label.cpu()))\n",
        "        eer_val += eer_result[0]/output[0].shape[0]\n",
        "        fpr_val += eer_result[2]\n",
        "        fnr_val += eer_result[3]\n",
        "\n",
        "        eer_threshold_val = eer_result[1]\n",
        "\n",
        "        curr_pred = (d.detach().cpu().numpy() >= 0.50)*1\n",
        "        curr_label = label.detach().cpu().numpy()\n",
        "        predicted_val.extend(curr_pred)\n",
        "        label_val.extend(curr_label)\n",
        "\n",
        "        ## Accuracy Calculation\n",
        "        acc_curr = np.sum(curr_pred == curr_label)/output[0].shape[0]\n",
        "        acc_val_avg += acc_curr\n",
        "\n",
        "        ## Increase Count\n",
        "        val_count += 1\n",
        "\n",
        "        ## Deleting\n",
        "        del output,x1,x2,length1,length2,label,loss_,curr_pred,curr_label,d,eer_result\n",
        "\n",
        "    # except Exception as e:\n",
        "    #   continue\n",
        "\n",
        "\n",
        "\n",
        "  acc_avg_test = 0\n",
        "  av_f1 = 0\n",
        "  av_prec = 0\n",
        "  av_recall = 0\n",
        "  test_count = 0\n",
        "\n",
        "  ### Testing Loop ###\n",
        "\n",
        "  with torch.no_grad() :\n",
        "    try :\n",
        "      for item in tqdm(test_loader) :\n",
        "        x1,x2,label,length1,length2 = item\n",
        "        x1 = x1.to(device)\n",
        "        x2 = x2.to(device)\n",
        "        label = label.to(device)\n",
        "        label = label.float()\n",
        "        optimizer.zero_grad()\n",
        "        model.eval()\n",
        "        output = model(x1,x2,length1,length2)\n",
        "        d = (cosine_similarity(output[0],output[1]) + 1)/2\n",
        "        loss_ = loss(d,label)\n",
        "        eer_result = calculate_eer(d.detach().cpu().numpy(),np.array(label.cpu()))\n",
        "        eer_test += eer_result[0]/output[0].shape[0]\n",
        "        fpr_test += eer_result[2]\n",
        "        fnr_test += eer_result[3]\n",
        "\n",
        "        eer_threshold_test = eer_result[1]\n",
        "\n",
        "        test_count += 1\n",
        "\n",
        "        threshold = 0.50\n",
        "        acc_avg_test += np.sum((d.detach().cpu().numpy() >= threshold)*1 == label.detach().cpu().numpy())/output[0].shape[0]\n",
        "        av_f1 += f1_score((d.detach().cpu().numpy() >= threshold)*1,label.detach().cpu().numpy())\n",
        "        av_prec += precision_score((d.detach().cpu().numpy() >= threshold)*1,label.detach().cpu().numpy())\n",
        "        av_recall += recall_score((d.detach().cpu().numpy() >= threshold)*1,label.detach().cpu().numpy())\n",
        "\n",
        "        del output,x1,x2,length1,length2,label,loss_,d,eer_result\n",
        "\n",
        "    except Exception as e:\n",
        "      continue\n",
        "\n",
        "  ## Saving Model Checkpoint\n",
        "  if path:\n",
        "    torch.save(model,os.path.join(os.getcwd(),path,'_model{}.pth').format(epoch))\n",
        "  else:\n",
        "    path = new_path\n",
        "    print(new_path)\n",
        "    os.mkdir(os.path.join(os.getcwd(),path))\n",
        "\n",
        "  ## Calculate f1 score\n",
        "  f1_train = f1_score(label_train,predicted_train)\n",
        "  prec_train = precision_score(label_train,predicted_train)\n",
        "  prec_val = precision_score(label_val,predicted_val)\n",
        "  recall_train = recall_score(label_train,predicted_train)\n",
        "  recall_val = recall_score(label_val,predicted_val)\n",
        "  f1_val = f1_score(label_val,predicted_val)\n",
        "\n",
        "  print(\"Precision train/val/test: {}/{}/{}\".format(prec_train,prec_val,av_prec/test_count))\n",
        "  print(\"Recall train/val/test: {}/{}/{}\".format(recall_train,recall_val,av_recall/test_count))\n",
        "  print(\"F1 train/val/test: {}/{}/{}\".format(f1_train,f1_val,av_f1/test_count))\n",
        "  print(\"EER Train/Val/Test: {}/{}/{}\".format(eer_train/train_count,eer_val/val_count,eer_test/test_count))\n",
        "  print(\"FPR Train/Val/Test: {}/{}/{}\".format(fpr_train/train_count,fpr_val/val_count,fpr_test/test_count))\n",
        "  print(\"FNR Train/Val/Test: {}/{}/{}\".format(fnr_train/train_count,fnr_val/val_count,fnr_test/test_count))\n",
        "\n",
        "  ## Logging into wandb\n",
        "  wandb.log({\"train_accuracy\": acc_train_avg/train_count,\"val_accuracy\": acc_val_avg/val_count,\"train_loss\": train_loss/train_count,\"val_loss\": val_loss/val_count,\"train_f1\": f1_train,\"val_f1\": f1_val,\"test_accuracy\":acc_avg_test/test_count,\"test_f1\":av_f1/test_count,\"train_eer\":eer_train/train_count,\"val_eer\":eer_val/val_count,\"test_eer\":eer_test/test_count, \"train_fpr\":fpr_train/train_count,\"val_fpr\":fpr_val/val_count,\"test_fpr\":fpr_test/test_count,\"train_fnr\":fnr_train/train_count,\"val_fnr\":fnr_val/val_count,\"test_fnr\":fnr_test/test_count,\"train_precision\":prec_train,\"val_precision\":prec_val,\"test_precision\":av_prec/test_count,\"train_recall\":recall_train,\"val_recall\":recall_val,\"test_recall\":av_recall/test_count,\"threshold\":threshold,\"epoch\":epoch,\"path\":path,\"f1_train\":f1_train,\"f1_val\":f1_val,\"f1_test\":av_f1/test_count,\"prec_train\":prec_train,\"prec_val\":prec_val,\"prec_test\":av_prec/test_count,\"recall_train\":recall_train,\"recall_val\":recall_val,\"recall_test\":av_recall/test_count,\"fpr_train\":fpr_train/train_count,\"fpr_val\":fpr_val/val_count,\"fpr_test\":fpr_test/test_count,\"fnr_train\":fnr_train/train_count,\"fnr_val\":fnr_val/val_count,\"fnr_test\":fnr_test/test_count})\n",
        "\n",
        "  ## Appending to list\n",
        "  val_loss_plot.append(val_loss/val_count)\n",
        "  loss_plot.append(train_loss/train_count)\n",
        "  iter.append(epoch)\n",
        "  train_eer_loss.append(acc_train_avg/train_count)\n",
        "  val_eer_loss.append(acc_val_avg/val_count)\n",
        "\n",
        "  print(\"---------------------------\")\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nMNzudp0sYo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
